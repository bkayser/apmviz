---
title: "Simple Stats"
author: "Bill Kayser"
date: "March 3, 2016"
output: 
  ioslides_presentation: 
    css: ./styles.css
    keep_md: no
    logo: images/logo_text.png
    smaller: yes
    widescreen: yes
---

## Estimating the Value of Data | What sort of insight does it provide?

```{r init_ss, echo=F, message=F, warning=F}
source('init.R')
```

* Does it quantify something we care about?
* Does it give us a qualitative assessment of the current status?
* Does it help us measure differences across similar things, like applications or servers?
* Does it help us identify trends and anomalies?
* Does it reveal underlying patterns or relationships in the data?

## Estimating the Cost

How cheaply can we collect the data?

* Is space or bandwidth a premium?
* Can it be reduced by combining values into a single value, or do we have to store every measurement?
* How much screen real estate does it take up?

The smaller the data the more things you can measure.

The less space it takes the more options you have for screen layout.

## Reducing Data

Can you reduce your measurement in X, $x_1 .. x_n$, by combining values using a reduce function, $f(a,b)$?

$$ X_{ab} = f(a, b)$$
$$ X_{cd} = f(c, d)$$
$$ X_{abcd} = f(X_{ab}, X_{cd})$$

Space required grows linearly with length of history; a simple time series

>* Min and Max: $min(X) = min\left(min(x_1, x_2), min(x_3, x_4), ... min(x_{n-1}, x_n)\right)$
>* Sum: $\sum_{i=1}^n x_i = (x_1 + x_2) + (x_3 + x_4) +...(x_{n-1} + x_n)$ 
>* Mean: $\bar{x} = \frac{1}{n}\sum_{i=0}^n x_i$ 
>* Standard Deviation: $\sigma = \sqrt{\frac{\sum_{i=0}^n x_i^2 - n\bar x }{n - 1}}$

## Hard To Reduce Data

Examples:

* Histograms (keep N buckets for each time period)
* Median
* Percentiles

The amount of storage required depends on the length of history 
but also the granularity of the data.

You need N buckets in each time period.

## Irreducible Data

Examples:

* Scatterplots
* Replays

Requires keeping a complete history of every measurement.

Space is proportional to the number of measures.

## Cost/Benefit Grid {.cvb}

```{r, fig.width=10, fig.height=6}
cvb_plot(cvb) +
    geom_tile(aes(x=7.0, y=7.0, width=4.5, height=4.5), 
              fill='#33FF33', alpha=0.2) +
    geom_text(aes(x=7.0, y=7.0), size=14,
              label="Sweet Spot") +
    geom_tile(aes(x=2.5, y=2.5, width=4, height=4), 
              fill='#FF3333', alpha=0.2) +
    geom_text(aes(x=2.5, y=2.5), size=12,
              label="Poor Return")

```

## Cost/Benefit - Mean Statistic {.cvb}

```{r, fig.width=10, fig.height=6}
cvb <- cvb_add(cvb, 'Mean', Efficiency=9, 2)
cvb_plot(cvb) +
    geom_tile(aes(x=7.0, y=7.0, width=4.5, height=4.5), 
              fill='#33FF33', alpha=0.2) +
    geom_text(aes(x=7.0, y=7.0), size=14,
              label="Sweet Spot") +
    geom_tile(aes(x=2.5, y=2.5, width=4, height=4), 
              fill='#FF3333', alpha=0.2) +
    geom_text(aes(x=2.5, y=2.5), size=12,
              label="Poor Return")

```

## Cost/Benefit - Scatterplot of Every Request  {.cvb}

```{r, fig.width=10, fig.height=6}
cvb <- cvb_add(cvb, 'Scatterplot', Efficiency=1.5, Value=9.5)
cvb_plot(cvb) +
    geom_tile(aes(x=7.0, y=7.0, width=4.5, height=4.5), 
              fill='#33FF33', alpha=0.2) +
    geom_text(aes(x=7.0, y=7.0), size=14,
              label="Sweet Spot") +
    geom_tile(aes(x=2.5, y=2.5, width=4, height=4), 
              fill='#FF3333', alpha=0.2) +
    geom_text(aes(x=2.5, y=2.5), size=12,
              label="Poor Return")

```

## Cost/Benefit - Histograms

```{r}
ggplot(microservice$events, aes(x=value)) + xlim(0, microservice$xlimit) +
    geom_histogram(bins=120) +
    my_plot_theme +
    summary_colorscale +
    summary_linescale +
   legend_right +
    theme(legend.position='none')

```


## Cost/Benefit - Histograms {.cvb}

```{r, fig.width=10, fig.height=6}
cvb <- cvb_add(cvb, 'Histogram', Efficiency=3.5, Value=8)
cvb_plot(cvb)
```


## Can we make use of the Standard Deviation?

```{r}
set.seed(100)
sample <- rnorm(10000,100, 40)
ymax <- 320

sample_summary <- data.frame(Measure=c('mean', 'median', 'q25', 'q75'),
                             V=c(mean(sample),
                                 median(sample),
                                 quantile(sample, 0.25),
                                 quantile(sample, 0.75)))
sample_sd <- sd(sample)

g_normal <- ggplot(data.frame(t=sample)) +
    aes(t) +
    geom_histogram(bins=120, fill='#CCCCCC') +
    geom_vline(data=sample_summary, aes(xintercept=V, color=Measure, linetype=Measure), size=1.2) +
    summary_colorscale + summary_linescale +
    my_plot_theme +
    coord_cartesian(xlim=c(0,240), ylim=c(0,ymax)) +
    legend_right

g_normal
```

## Estimating Quartiles Using Standard Deviation

```{r}
m <- mean(sample)

z <- qnorm(p=0.75)

estIQ <- data.frame(z67=c(m-sample_sd*z, m+sample_sd*z))

g_est_quartiles <- g_normal + 
    geom_rect(data=estIQ,
              aes(xmin=min(z67),
                  xmax=max(z67)),
              inherit.aes = F,
              ymax=ymax,
              ymin=0,
              fill='red',
              alpha=0.1) +
    geom_vline(data=estIQ,
               aes(xintercept=z67),
               size=1.5, color='black', linetype='dotted') 

g_est_quartiles

```

## Estimating Quartiles Using Standard Deviation

If response times had a normal distribution we could show everything using
simple statistics and estimated percentiles:

```{r}
sample_t <- data.frame(t=100+sample, x=1:100)
sample_summary <- group_by(sample_t, x) %>%
    summarize(mean=mean(t), q25_est=mean-(z*sd(t)),
              q75_est=mean+(z*sd(t)))
    
ggplot(sample_summary) +
    aes(x=x, ymin=q25_est, ymax=q75_est, y=mean) +
    geom_ribbon(aes(color='q75',
                    linetype='q75'),
                fill=region_fill,
                alpha=0.8,
                size=1.5) +
    geom_point(data=sample_t, inherit.aes=F, aes(x=x,y=t), alpha=0.2) + 
    summary_colorscale + summary_linescale +
    my_plot_theme +
    theme(legend.position='none')
```

## Estimating Quartiles

Summary Statistics Don't Work on Non-Gassian Data

```{r}
scdata <- read_transactions('./data/timeseries/news-site.rds', 'frontend')
est_band <- dcast(scdata$plotlines, timestamp ~ measure)
g_scatter <- scatterplot(scdata)
g_scatter +
    geom_ribbon(data=est_band, inherit.aes = F,
                aes(x=timestamp,
                    ymax=q75_est,
                    ymin=q25_est),
                fill=region_fill,
                alpha=0.4) +
    geom_line(data=filter(scdata$plotlines, measure %in% c('q25', 'q75')), 
              aes(x=timestamp, y=value, color=measure, linetype=measure),
              size=1.5) +
    summary_colorscale + summary_linescale +
    theme(legend.position = "top",
          legend.key.width=unit(2, 'cm')) +
        theme(legend.position='none') 


```

## Cost/Benefit - Standard Deviation {.cvb}

```{r, fig.width=10, fig.height=6}
cvb <- cvb_add(cvb, 'Std. Dev.', Efficiency=8.5, Value=1)
cvb_plot(cvb)
```

## Mean vs Median | Non-gaussian distributions

```{r}

m_colorscale <- scale_color_manual(values=c(mean='red', median='red'))
ex <- read_transactions('./data/timeseries/supplychain.rds', 'frontend')
add_plotlines(scatterplot(ex), ex) + blank_theme + m_colorscale 

```

## Mean vs Median | Non-gaussian distributions

```{r}
ex <- read_transactions('./data/timeseries/storefront-interrupt.rds', 'backend')
add_plotlines(scatterplot(ex), ex) + blank_theme + m_colorscale
```

## Mean vs Median | Non-gaussian distributions
```{r}
ex <- read_transactions('./data/timeseries/news-site.rds', 'frontend')
add_plotlines(scatterplot(ex), ex)  + blank_theme + m_colorscale

```

## Mean vs Median | Non-gaussian distributions

```{r}
ex <- read_transactions('./data/timeseries/microservice.rds', 'duration')
add_plotlines(scatterplot(ex), ex)  + blank_theme + m_colorscale
```

## Cost/Benefit - Median {.cvb}

```{r, fig.width=10, fig.height=6}
cvb <- cvb_add(cvb, 'Median/Percentiles', Efficiency=4, Value=6.5)
cvb_plot(cvb)
```

